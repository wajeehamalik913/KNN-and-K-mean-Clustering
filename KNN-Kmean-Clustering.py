# -*- coding: utf-8 -*-
"""i181587_A04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F8o4owaixalqVXcmZSe9CCsaDs_Mn75I
"""

from google.colab import drive
drive.mount('/content/drive')

import csv
import pandas as pd
import copy
import math 
import numpy as np

data = pd.read_csv("/content/drive/My Drive/dataset.csv")
data = data.sample(n = 800)
tempdata=copy.deepcopy(data)
data.drop(data.columns[[3]], axis = 1, inplace = True)
train = data.sample(frac=0.8,random_state=200)
test = data.drop(train.index)
#print(test)

pred_code=[]

orig_code=[]
k=3
def most_frequent(List):
    counter = 0
    num = 0
      
    for i in range(len(List)):
        curr_frequency = List.count(List[i])
        if(curr_frequency> counter):
            counter = curr_frequency
            num = i
  
    return num
def euclidean_distance(test1,test2,train1,train2):
        temp=(((test1-test2)**2)+((train1-train2)**2))
        temp2=math.sqrt(temp)
        #print("Distance:",temp2)
        return temp2
for i in range(len(test)):
        orig_code.append(test.values[i][0])
for i in range(len(test)):
    distances=[]
    neig_dis=[]
    code=[]
    cp=test.values[i][1]
    oc=test.values[i][2]
    for j in range(len(train)):
        temp=euclidean_distance(cp,train.values[j][1],oc,train.values[j][2])
        t=(temp,train.values[j][0])
        #print("TRAINNNN:",train.values[j][0])
        distances.append(t)
    distances.sort()
    for l in range(k):
        List=distances[l]
        neig_dis.append(List[0])
        code.append(List[1])
    n=np.argmax(code)
    #print("codee   ",code[n],"          ",code)
    pred_code.append(code[n])

classes = np.unique(orig_code)
number_of_classes = len(classes)

conf_matrix = pd.DataFrame(
    np.zeros((number_of_classes, number_of_classes),dtype=int),
    index=classes,
    columns=classes)

for i, j in zip(orig_code,pred_code):
    conf_matrix.loc[i,j] += 1

print("**************************************")
print(conf_matrix.values)

print("**************************************")
mat=conf_matrix.copy()
mat=mat.to_numpy()
TP_sum = np.trace(mat)
print(TP_sum)
acc=TP_sum/(len(orig_code))
print( "Accuracy:",acc)
FP_sum=0
from sklearn.metrics import accuracy_score
print(accuracy_score(orig_code, pred_code))
print("**************************************")
dimen=mat.shape

for i in range(dimen[0]):
    for j in range(dimen[1]):
            if(i!=j): 
                FP_sum+=mat[j][i]
    precision=(mat[i][i]/(mat[i][i]+FP_sum))
    print("Precision",i,":",precision)

print("**************************************")
for i in range(dimen[0]):
    for j in range(dimen[1]):
            if(i!=j): 
                FP_sum+=mat[i][j]
    precision=(mat[i][i]/(mat[i][i]+FP_sum))
    print("Recall",i,":",precision)

print("**************************************")
from sklearn.metrics import recall_score
print(recall_score(orig_code, pred_code, average=None))

print("**************************************")
#for i in range(len(distances)):
    #print("DISSS:",distances[i])
#t=0;
#f=0;
#for i in range(len(orig_code)):
#    if(orig_code[i]==pred_code[i]):
#        t+=1
 #   else:
  #      f+=1
#for i in range(len(orig_code)):
 #   print("original value:",orig_code[i],"        predicted:",pred_code[i])

#print("*************PRECISION****************")
#precision = (t / (t+f) )
#print(precision)
#print("**************************************")





#print(tempdata)
tempdata.drop(tempdata.columns[[3]], axis = 1, inplace = True)
#print(tempdata)
tempdata.drop(tempdata.columns[[0]], axis = 1, inplace = True)
print(tempdata)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from numpy import random
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

"""### Cluster assignment algorithm"""

# Assign every training example x^(i) to its closest centroid, given the current centroid positions
def find_closest_centroids(X, centroids):
    '''returns the array of assigned clusters to each example ''' 
    m = X.shape[0] # no of datappints
    k = centroids.shape[0]  #division size
    idx = np.zeros(m) # array to assign the centriod
    
    # Your Code Here
    for i in range(m):
        distances = np.linalg.norm(X[i] - centroids, axis=1)
        min_dst = np.argmin(distances)
        idx[i] = min_dst
    
    return idx

"""### Computing centoroid means"""

# Update Centroids
def compute_centroids(X, idx, k):
    m, n = X.shape
    centroids = np.zeros((k, n))  #3*4
    '''Return Updated Values of all K centroids'''
    
    # Your Code Here
    for k in range(k):
        centroids[k, :] = np.mean(X[idx.ravel() == k, :], axis=0)
    
    return centroids

"""### Putting it all together (full algorithm: cluster assignment and re-computing centroids)"""

def run_k_means(X, initial_centroids, max_iters):
    m, n = X.shape
    k = initial_centroids.shape[0]
    idx = np.zeros(m)
    centroids = initial_centroids
    
    for i in range(max_iters):
        # find closest centroid
        idx = find_closest_centroids(X, centroids)
        #update centroids
        centroids = compute_centroids(X, idx, k)
    
    return idx, centroids

"""#### Now we will shuffle our data so that the data point are not in order, and save the clusters for visualization"""

import copy
tempdata=tempdata.to_numpy()
X=tempdata
np.random.shuffle(X)
cluster1 = X[0:50,:]
cluster2 = X[50:100,:]
cluster3 = X[100:150,:]

"""#### Now let's run algorithim and form k-means clusters from random initial clusters"""

initial_centroids = initial_centroids = np.array([X[random.randint(0,50)], X[random.randint(50,100)], X[random.randint(100,150)]])
# Select k=3 , pick 3 random centroids from data
idx, centroids = run_k_means(X, initial_centroids, 2000)# use 1000 iterations to perform clustering

import sklearn
from sklearn.cluster import KMeans

sklearn.metrics.davies_bouldin_score(X, idx)

"""#### Save the clusters form by the k-mean for further visualization"""

k_Meancluster1 = X[np.where(idx == 0)[0],:]
k_Meancluster2 = X[np.where(idx == 1)[0],:]
k_Meancluster3 = X[np.where(idx == 2)[0],:]
i = compute_DB_index(X,orig_code, k_Meancluster1, 3)
ii = compute_DB_index(X,orig_code, k_Meancluster2, 3)
iii = compute_DB_index(X,orig_code, k_Meancluster3, 3)
print((i+ii+iii)/3)

"""## 5: Add a visualisation
Identify a suitable library to depict both your inputs as well as the results of k-means clustering applied to those inputs.

## Using Matplotlib

### First Lets see the initial input with random clusters
"""

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(cluster1[:,0], cluster1[:,1], s=30, color='r', label='Cluster 1')
ax.scatter(cluster2[:,0], cluster2[:,1], s=30, color='g', label='Cluster 2')
ax.scatter(cluster3[:,0], cluster3[:,1], s=30, color='b', label='Cluster 3')
ax.legend()

"""### Now let's visuaize the Clusters form by our k-mean"""

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(k_Meancluster1[:,0], k_Meancluster1[:,1], s=30, color='r', label='Cluster 1')
ax.scatter(k_Meancluster2[:,0], k_Meancluster2[:,1], s=30, color='g', label='Cluster 2')
ax.scatter(k_Meancluster3[:,0], k_Meancluster3[:,1], s=30, color='b', label='Cluster 3')
ax.legend()